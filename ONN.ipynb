{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import fsolve\n",
    "from scipy.special import softmax\n",
    "import pandas as pd\n",
    "#from numpy import vstack\n",
    "#from numpy import argmax\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "def OSA(X):  #Optical Saturable Absorber, numerically returns Tm value given input intensity vector.\n",
    "        T0 = 0.9\n",
    "        Tm = X\n",
    "        sigma = 10e-15\n",
    "        tau = 10e-9\n",
    "        c=0\n",
    "        \n",
    "        for i in X:\n",
    "            neg_flag = 0\n",
    "            if i < 0:\n",
    "                i0 = -i\n",
    "                neg_flag = 1\n",
    "            elif i == 0:\n",
    "                i0 = 0.1\n",
    "            else:\n",
    "                i0 = i\n",
    "\n",
    "            func = lambda Tmp : i0*sigma*tau - 0.5*(np.log((Tmp/T0)))/(1.0 - Tmp) \n",
    "            Tm_guess = 0.1\n",
    "            if neg_flag == 0:\n",
    "                Tm[c] = fsolve(func, Tm_guess)\n",
    "                c=c+1\n",
    "            else:\n",
    "                Tm[c] = -1 * fsolve(func, Tm_guess)\n",
    "                c=c+1\n",
    "        return Tm\n",
    "\n",
    "\n",
    "#Forward Propagation\n",
    "\n",
    "\n",
    "def forward(X,w1,w2):\n",
    "    a1= w1.dot(X)\n",
    "    z1=OSA(a1)\n",
    "    a2=w2.dot(z1)\n",
    "    z2=OSA(a2)\n",
    "    h=softmax(abs(a2))\n",
    "    return h, z1, z2, a1, a2\n",
    "\n",
    "def dactivation(Tm, I):      #Given Tm and Intensity, returns dactivation for Backpropagation update algorithm\n",
    "    sigma = 10e-15\n",
    "    tau = 10e-9\n",
    "    B = 2 * sigma * tau\n",
    "    return (Tm * B * (1 - Tm))/(B * I * Tm + 1)\n",
    "\n",
    "def backward(h,targets, input, z1, z2, a1, a2, dact1, dact2, w2):  #return backward propagation of errors\n",
    "    dw2 = np.outer((h-targets) @ np.sign(h) * dact2, z1)\n",
    "    dw1 = np.outer(((h-targets) @ np.sign(h) * dact2 @ w2) * dact1, input)\n",
    "    return dw1 , dw2\n",
    "\n",
    "def gradient(w1, w2, dw1, dw2, stepsize):\n",
    "    nw1 = w1 - stepsize * dw1\n",
    "    nw2 = w2 - stepsize * dw2\n",
    "    return nw1, nw2\n",
    "\n",
    "def encoder(target):\n",
    "    t=np.zeros(11)\n",
    "    t[target]= 1\n",
    "    return t\n",
    "\n",
    "def train_batch(X, w1, w2, batch_size, nhid, stepsize):  #train on a batch\n",
    "\n",
    "    avg_w1 = np.zeros((nhid, 10))\n",
    "    avg_w2 = np.zeros((11, nhid))\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        [h, z1, z2, a1, a2] = forward(X[i,:-1], w1, w2)\n",
    "        dact1 = dactivation(z1, a1)\n",
    "        dact2 = dactivation(z2, a2)\n",
    "        t = encoder(X[i,-1].astype(int))\n",
    "        [dw1, dw2] = backward(h, t, X[i,:-1], z1, z2, a1, a2, dact1, dact2, w2)\n",
    "        avg_w1=avg_w1+dw1\n",
    "        avg_w2=avg_w2+dw2\n",
    "\n",
    "    avg_w1=avg_w1/batch_size\n",
    "    avg_w2=avg_w2/batch_size\n",
    "    [nw1, nw2] = gradient(w1,w2,avg_w1,avg_w2, stepsize)\n",
    "    return nw1, nw2\n",
    "\n",
    "def train_model(nhid, batch_size, stepsize, epochs):\n",
    "\n",
    "    #Initialization\n",
    "    w1 = np.array(np.random.rand(nhid,10)-0.5)\n",
    "    w2 = np.array(np.random.rand(11,nhid)-0.5)\n",
    "\n",
    "    #Reading dataset from .csv files\n",
    "\n",
    "    dataset=pd.read_csv('train.csv', sep=',',header=None)\n",
    "\n",
    "    batch_per_epoch = 6\n",
    "\n",
    "    input=np.array(dataset)\n",
    "  \n",
    "\n",
    "      \n",
    "    for j in range(epochs):\n",
    "        for b in range(batch_per_epoch):\n",
    "            [w1, w2] = train_batch(input[(batch_size*b):(batch_size*(b+1)),:], w1, w2, batch_size, nhid, stepsize)\n",
    "        \n",
    "\n",
    "    return w1, w2\n",
    "\n",
    "\n",
    "def validator(w1, w2):\n",
    "\n",
    "    df=pd.read_csv('train.csv', sep=',',header=None)\n",
    "    inp=np.array(df)\n",
    "\n",
    "    correct = 0\n",
    "    indx=np.zeros(528)\n",
    "    for i in range(20):\n",
    "       [hj, g1, g2, x1, x2] = forward(inp[i,:-1],w1,w2)\n",
    "       \n",
    "       \n",
    "       indx[i]=np.argmax(abs(x2)) \n",
    "       print(inp[i,-1].astype(int))\n",
    "       print(np.argmax(abs(x2)))\n",
    "       if np.argmax(abs(x2)) == (inp[i,-1].astype(int)):\n",
    "          correct = correct +1\n",
    "           \n",
    "    \n",
    "    return correct/20\n",
    "\n",
    "def predict(input, w1,w2):\n",
    "\n",
    "    [h, z1, z2, a1, a2] = forward(input,w1,w2)\n",
    "    \n",
    "    return h\n",
    "\n",
    "\n",
    "stepsize = 0.1\n",
    "batch_size = 88\n",
    "nhid = 3 #number of the neurons in the hidden layer\n",
    "epochs = 80\n",
    "\n",
    "[w1, w2] = train_model(nhid, batch_size, stepsize, epochs)\n",
    "\n",
    "test=[-3.86, 2.11, -0.939,\t0.688,\t-0.675,\t1.679,\t-0.512,\t0.928,\t-0.167,\t-0.434]\n",
    "\n",
    "\n",
    "\n",
    "print(validator(w1,w2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
